{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from PCHID.dataset.dataset import *\n",
    "from PCHID.utility.utils import *\n",
    "from PCHID.domains.gridworld import *\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001  # learning rate\n",
    "EPSILON = 0.1  # greedy policy\n",
    "GAMMA = 0.9  # reward discount\n",
    "TARGET_REPLACE_ITER = 100  # target update frequency\n",
    "MEMORY_CAPACITY = 30000\n",
    "EPISODES = 500\n",
    "\n",
    "IMAGE_SIZE = 16\n",
    "N_ACTIONS = 8\n",
    "N_STATES = 2 + IMAGE_SIZE * IMAGE_SIZE\n",
    "FILE_NAME = '_size16_50obs_5000dom'\n",
    "DOMAIN_NUM = 5000\n",
    "K = 20\n",
    "\n",
    "\n",
    "class conf():\n",
    "    def __init__(self,\n",
    "                 datafile = 'PCHID/dataset/gridworld_RL_{0}x{1}'.format(IMAGE_SIZE, IMAGE_SIZE) + FILE_NAME ,\n",
    "                 image_size = IMAGE_SIZE,\n",
    "                 lr = 0.005,\n",
    "                 epochs = 30,\n",
    "                 k = K,\n",
    "                 l_i = 2,\n",
    "                 l_h = 150,\n",
    "                 l_q = 10,\n",
    "                 batch_size = 128,\n",
    "                 DOMAIN_NUM = DOMAIN_NUM,\n",
    "                 algorithm = 'K_step',\n",
    "                 experiment = '2step'):\n",
    "        self.domain_num = DOMAIN_NUM\n",
    "        self.l_i = l_i\n",
    "        self.l_h = l_h\n",
    "        self.image_size = image_size\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.k = k\n",
    "        self.l_q = l_q\n",
    "        self.batch_size = batch_size\n",
    "        self.datafile = datafile\n",
    "        self.algorithm = algorithm\n",
    "        self.experiment = experiment\n",
    "        self.file_name = FILE_NAME\n",
    "\n",
    "\n",
    "class GW_env():\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        self.R, self.goal = self.X2R(X)\n",
    "        self.G = gridworld(1 - X[0], self.goal[0], self.goal[1])\n",
    "        self.actions = np.asarray([[-1, 0], [1, 0], [0, 1], [0, -1], [-1, 1], [-1, -1], [1, 1], [1, -1]])\n",
    "\n",
    "    def X2R(self, X):\n",
    "        goal = [np.argmax(X[1]) // config.image_size, np.argmax(X[1]) % config.image_size]\n",
    "        G = gridworld(1 - X[0], goal[0], goal[1])\n",
    "        R = X[1] - (1 - X[0]) * 0.02 - 2 * X[0]\n",
    "        return R, goal\n",
    "\n",
    "    def reset(self):\n",
    "        self.states_xy, self.states_one_hot = sample_trajectory(self.G, 1)\n",
    "        if len(self.states_xy[0]) > 0:\n",
    "            self.s = self.states_xy[0][0]\n",
    "            self.bestlen = len(self.states_xy[0])\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "    def step(self, a):\n",
    "        s_0 = self.s.copy()\n",
    "        self.s = self.s + self.actions[a]\n",
    "        if self.s[0] >= config.image_size - 1:\n",
    "            self.s[0] = config.image_size - 1\n",
    "        if self.s[0] <= 0:\n",
    "            self.s[0] = 0\n",
    "        if self.s[1] >= config.image_size - 1:\n",
    "            self.s[1] = config.image_size - 1\n",
    "        if self.s[1] <= 0:\n",
    "            self.s[1] = 0\n",
    "        reward = self.reward()\n",
    "        ifdone = self.ifdone()\n",
    "        if self.X[0][tuple(self.s)] == 1:  # target s is an obstacle\n",
    "            self.s = s_0.copy()\n",
    "        return self.s, reward, ifdone, 0\n",
    "\n",
    "    def reward(self):\n",
    "        if self.R[int(self.s[0]), int(self.s[1])] == -2:\n",
    "            return -0.02\n",
    "        else:\n",
    "            return self.R[int(self.s[0]), int(self.s[1])]\n",
    "\n",
    "    def ifdone(self):\n",
    "        if self.s[0] == self.goal[0] and self.s[1] == self.goal[1]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "class VIN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VIN, self).__init__()\n",
    "        self.config = config\n",
    "        self.h = nn.Conv2d(in_channels=1, out_channels=config.l_h, kernel_size=(3, 3), stride=1, padding=1, bias=True)\n",
    "        self.r = nn.Conv2d(in_channels=config.l_h, out_channels=1, kernel_size=(1, 1), stride=1, padding=0, bias=False)\n",
    "        self.q = nn.Conv2d(in_channels=1, out_channels=config.l_q, kernel_size=(3, 3), stride=1, padding=1, bias=False)\n",
    "        self.fc = nn.Linear(in_features=config.l_q, out_features=8, bias=False)\n",
    "        self.w = Parameter(torch.zeros(config.l_q, 1, 3, 3), requires_grad=True)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, S, config):\n",
    "        S = S.reshape([-1, N_STATES])\n",
    "        X = S[:, :config.image_size**2].reshape([-1, 1, config.image_size, config.image_size])\n",
    "        S1 = S[:, config.image_size**2:config.image_size**2 + 1].long().squeeze(1)\n",
    "        S2 = S[:, config.image_size**2 + 1:].long().squeeze(1)\n",
    "        h = self.h(X)\n",
    "        r = self.r(h)\n",
    "        q = self.q(r)\n",
    "        v, _ = torch.max(q, dim=1, keepdim=True)\n",
    "        for i in range(0, config.k - 1):\n",
    "            q = F.conv2d(torch.cat([r, v], 1), torch.cat([self.q.weight, self.w], 1), stride=1, padding=1)\n",
    "            v, _ = torch.max(q, dim=1, keepdim=True)\n",
    "\n",
    "        q = F.conv2d(torch.cat([r, v], 1), torch.cat([self.q.weight, self.w], 1), stride=1, padding=1)\n",
    "        slice_s1 = S1.long().expand(config.image_size, 1, config.l_q, q.size(0))\n",
    "        slice_s1 = slice_s1.permute(3, 2, 1, 0)\n",
    "        q_out = q.gather(2, slice_s1).squeeze(2)\n",
    "\n",
    "        slice_s2 = S2.long().expand(1, config.l_q, q.size(0))\n",
    "        slice_s2 = slice_s2.permute(2, 1, 0)\n",
    "        q_out = q_out.gather(2, slice_s2).squeeze(2)\n",
    "\n",
    "        logits = self.fc(q_out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net = VIN(config).cuda()\n",
    "        self.target_net = VIN(config).cuda()\n",
    "        self.learn_step_counter = 0  # for target updating\n",
    "        self.memory_counter = 0  # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))  # initailize memory\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_decay = 1 / 5e6\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        if self.epsilon > 0.2:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        if np.random.uniform() > self.epsilon:  # greedy\n",
    "            actions_value = self.eval_net.forward(x.cuda(), config)\n",
    "            action = torch.max(actions_value, 1)[1].data.cpu().numpy()[0]\n",
    "        else:\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target net update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).cuda()\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES + 1].astype(int)).cuda()\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES + 1:N_STATES + 2]).cuda()\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).cuda()\n",
    "        q_eval = self.eval_net(b_s.cuda(), config).gather(1, b_a)\n",
    "        q_next = self.target_net(b_s_.cuda(), config).detach()\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].reshape([BATCH_SIZE, 1])\n",
    "        loss = self.loss_func(q_eval.cuda(), q_target.cuda())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def learn_pchid(self, batch_size, step_num, ier_buffer, optimizer_imitation):\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        if ier_buffer.length(step_num) == 0:\n",
    "            return None\n",
    "        if batch_size > ier_buffer.length(step_num):\n",
    "            return None\n",
    "        state, action = ier_buffer.sample(batch_size, step_num)\n",
    "        state = torch.FloatTensor(state).cuda()\n",
    "        action_target = torch.LongTensor(action).cuda()\n",
    "        action_pred = self.eval_net(state, config)\n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        loss = loss_func(action_pred, action_target)\n",
    "        optimizer_imitation.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_imitation.step()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = {'1step': deque(maxlen=capacity)}\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, action, step_num):\n",
    "        try:\n",
    "            self.buffer[step_num]\n",
    "        except:\n",
    "            self.buffer[step_num] = deque(maxlen=self.capacity)\n",
    "        self.buffer[step_num].append((state, action))\n",
    "\n",
    "    def sample(self, batch_size, step_num):\n",
    "        state, action = zip(*random.sample(self.buffer[step_num], batch_size))\n",
    "        return np.stack(state), action\n",
    "\n",
    "    def length(self, step_num):\n",
    "        try:\n",
    "            self.buffer[step_num]\n",
    "        except:\n",
    "            return 0\n",
    "        return len(self.buffer[step_num])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def test_isvalid_multistep(step_length, state_start, environment_start,config,dqn):\n",
    "    if step_length == 1:\n",
    "        return True\n",
    "    env_tim = deepcopy(environment_start)\n",
    "    state_tim = torch.as_tensor(deepcopy(state_start))\n",
    "    for step_i in range(step_length - 1):\n",
    "        a = dqn.eval_net(state_tim.cuda(), config).detach()\n",
    "        a = torch.max(a, 1)[1].data.cpu().numpy()[0]\n",
    "        next_state_tim, r_tim, done_tim, info_tim = env_tim.step(a)\n",
    "        next_state_tim = torch.cat([torch.as_tensor(env_tim.R.flatten()), torch.as_tensor(next_state_tim).float()], 0)\n",
    "        next_state_tim[np.where(env_tim.R.flatten() > 9)[0][0]] = -0.02\n",
    "        next_state_tim[np.where(state_tim.flatten()[:-2] > 9)[0][0]] = 9.98\n",
    "\n",
    "        if next_state_tim.numpy()[-2] * 8 + next_state_tim.numpy()[-1] == np.where(\n",
    "                next_state_tim.numpy()[:-2] > 9)[0][0]:\n",
    "            return False\n",
    "        state_tim = next_state_tim\n",
    "    return True\n",
    "\n",
    "\n",
    "config = conf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Samples: 9866\n",
      "(9866, 2, 16, 16)\n",
      "X_train.shape (9866, 2, 16, 16)\n",
      "X_set.shape (2017, 2, 16, 16)\n",
      "\n",
      "Collecting experience...\n",
      "0 epsilon_now 0.1 Saving Models...\n",
      "epsilon REVISED 0.1\n",
      "[]\n",
      "ier length 10 3 0 0 0\n",
      "[]\n",
      "ier length 41 5 0 0 0\n",
      "[]\n",
      "ier length 81 6 0 0 0\n",
      "[]\n",
      "ier length 103 7 0 0 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5237d6272985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_counter\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#i_episode<= 50:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHorizon_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8019cd04fbf8>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mb_s_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mN_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mq_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mq_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_s_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mq_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_r\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mGAMMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8019cd04fbf8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, S, config)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as opt\n",
    "import matplotlib.pyplot as plt\n",
    "from PCHID.dataset.dataset import GridworldData\n",
    "for repeat in range(10):\n",
    "    horizon_history = []\n",
    "    # load train dataset\n",
    "    trainset = GridworldData(config.datafile + '_repeat{}.npz'.format(repeat), image_size=config.image_size, train=True, transform=None)\n",
    "    print(trainset.images.shape)\n",
    "    X_train = trainset.images\n",
    "    X_set = [X_train[0]]\n",
    "    for i in range(1, len(X_train)):\n",
    "        if False in (set((X_train[i] == X_set[-1]).flatten())):\n",
    "            X_set.append(X_train[i])\n",
    "    print(\"X_train.shape\", X_train.shape)\n",
    "    print(\"X_set.shape\", np.shape(X_set))\n",
    "\n",
    "    dqn = DQN()\n",
    "    optimizer_imitation = opt.Adam(dqn.eval_net.parameters(), lr=3e-4)\n",
    "\n",
    "    Horizon_list = [1,2]\n",
    "    losses = [[] for i in range(len(Horizon_list)) ]\n",
    "    q_value_buffer = []\n",
    "    los_lst = []\n",
    "    ier_buffer = ReplayBuffer(50000)\n",
    "\n",
    "    episodes = []\n",
    "    scores = []\n",
    "    print('\\nCollecting experience...')\n",
    "    q_value_buffer_sub = []\n",
    "    for i_episode in range(EPISODES):\n",
    "        score = 0\n",
    "        X = X_set[i_episode]\n",
    "        env = GW_env(X)\n",
    "        if i_episode % 20 == 0:\n",
    "            print(i_episode, 'epsilon_now', dqn.epsilon, 'Saving Models...')\n",
    "            if True:\n",
    "                print('epsilon REVISED', dqn.epsilon)\n",
    "\n",
    "            name_total = '16_16_ablation/' + '{}/'.format(\n",
    "                config.algorithm) + str(i_episode) + config.file_name + '_repeat{}'.format(repeat) + '{}.pth'.format(config.experiment)\n",
    "            torch.save(dqn.target_net.state_dict(), name_total)\n",
    "\n",
    "        env.reset()\n",
    "        s = env.s\n",
    "        s = torch.cat([torch.as_tensor(env.R).flatten(), torch.as_tensor(s).float()], 0).cuda()\n",
    "        step = 1\n",
    "        episode = []\n",
    "        env_list = []\n",
    "        while True:\n",
    "            step += 1\n",
    "            a = dqn.choose_action(s)\n",
    "            if len(Horizon_list) >= 2:\n",
    "                env_list.append(deepcopy(env))\n",
    "\n",
    "            # take action\n",
    "            s_, r, done, info = env.step(a)\n",
    "            score += r\n",
    "            s_ = torch.cat([torch.as_tensor(env.R.flatten()), torch.as_tensor(s_).float()], 0).cuda()\n",
    "\n",
    "            if step >= 50:\n",
    "                done = True\n",
    "\n",
    "            episode.append((s.cpu().numpy(), a, r, s_.cpu().numpy()))\n",
    "            if done:\n",
    "                for ind, (state, action, reward, next_state) in enumerate(episode):\n",
    "                    if len(Horizon_list) >= 2:\n",
    "                        assert len(env_list) == len(episode)\n",
    "\n",
    "                    for t_ in Horizon_list:\n",
    "                        try:\n",
    "                            episode[t_ + ind]\n",
    "                        except:\n",
    "                            continue\n",
    "                        target_state_idx = int(config.image_size * episode[t_ + ind][-4][-2] + episode[t_ + ind][-4][-1])#np.where(episode[t_ + ind][-4][:-2] > 9)[0][0]\n",
    "                        state_ = deepcopy(state)\n",
    "                        state_[np.where(state_[:-2] > 9)[0][0]] = -0.02\n",
    "                        state_[target_state_idx] = 9.98\n",
    "\n",
    "                        if len(Horizon_list) >= 2:\n",
    "                            if np.max([np.abs(state_[-2] - np.where(state_[:-2] > 9)[0][0]//config.image_size),np.abs(state_[-1] - np.where(state_[:-2] > 9)[0][0]%config.image_size)]) >=t_:\n",
    "                                ier_buffer.push(state_, action, str(t_) + 'step')\n",
    "                        else:  # 1-step\n",
    "                            if state_[-2] != next_state[-2] or state_[-1] != next_state[-1]:\n",
    "                                ier_buffer.push(state_, action, str(t_) + 'step')\n",
    "\n",
    "                    if np.random.uniform() < 0.8:\n",
    "                        for t_ in np.random.choice(len(episode) - ind, 1):\n",
    "                            try:\n",
    "                                episode[t_ + ind]\n",
    "                            except:\n",
    "                                continue\n",
    "                            target_state_idx = int(config.image_size * episode[t_ + ind][-4][-2] + episode[t_ + ind][-4][-1])#np.where(episode[t_ + ind][-4][:-2] > 9)[0][0]\n",
    "                            state_ = deepcopy(state)\n",
    "                            state_[np.where(state_[:-2] > 9)[0][0]] = -0.02\n",
    "                            state_[target_state_idx] = 9.98\n",
    "\n",
    "                            next_state_ = deepcopy(next_state)\n",
    "                            next_state_[np.where(next_state_[:-2] > 9)[0][0]] = -0.02\n",
    "                            next_state_[target_state_idx] = 9.98\n",
    "\n",
    "                            r_temp = reward\n",
    "                            if next_state_[-2] * config.image_size + next_state_[-1] == target_state_idx:\n",
    "                                r_temp = 9.98\n",
    "                            dqn.store_transition(state_, action, r_temp, next_state_)\n",
    "                    else:\n",
    "                        dqn.store_transition(state, action, reward, next_state)\n",
    "\n",
    "                        \n",
    "                        #pass\n",
    "                episodes.append(i_episode)\n",
    "                scores.append(score)\n",
    "                q_value_buffer_sub.append(step / env.bestlen)\n",
    "\n",
    "                break\n",
    "            s = s_\n",
    "\n",
    "            if dqn.memory_counter > BATCH_SIZE:\n",
    "                dqn.learn()\n",
    "                if True:  #i_episode<= 50:\n",
    "                    for h in Horizon_list:\n",
    "                        los_lst = []\n",
    "                        flag = 0\n",
    "                        loss1 = dqn.learn_pchid(BATCH_SIZE, '{}step'.format(h), ier_buffer, optimizer_imitation)\n",
    "                        if loss1 is not None:\n",
    "                            flag = 1\n",
    "                            losses[h-1].append(loss1.item())\n",
    "                            los_lst.append('loss{}'.format(h))\n",
    "\n",
    "        \n",
    "        print(los_lst)\n",
    "        print('ier length', ier_buffer.length('1step'),ier_buffer.length('2step'),ier_buffer.length('3step'),ier_buffer.length('4step'),ier_buffer.length('5step'))\n",
    "    q_value_buffer.append(q_value_buffer_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
